#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Å "–ø–æ–¥–∞—Ä–∫–∏"
"""
import re
from typing import Set

def _normalize_letters(s: str) -> str:
    return (
        s.lower()
        .replace('—ë', '–µ')
        .replace('—ç', '–µ')
        .replace('-', '')
    )

def tokenize(s: str) -> Set[str]:
    s = _normalize_letters(s)
    toks = re.findall(r"[\w–∞-—è–ê-–Ø]+", s)
    stop = {"–∏", "–≤", "–Ω–∞", "–ø–æ"}
    return {t for t in toks if t and t not in stop}

def test_category_matching():
    # –†–µ–∞–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –±–∞–∑—ã
    categories = [
        "üéÅ Gifts",
        "üé≠ Entertainment", 
        "üçΩÔ∏è –ö–∞—Ñ–µ –∏ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã",
        "üí∞ –ü—Ä–æ—á–∏–µ —Ä–∞—Å—Ö–æ–¥—ã",
        "üõí Products"
    ]
    
    # –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    searches = [
        "–ø–æ–¥–∞—Ä–∫–∏",
        "–ü–æ–¥–∞—Ä–∫–∏",
        "–ø–æ–¥–∞—Ä–æ–∫",
        "gifts",
        "–∫–∞—Ñ–µ",
        "–∫–∞—Ñ–µ –∏ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã",
        "—Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã",
        "–ø—Ä–æ–¥—É–∫—Ç—ã",
        "products"
    ]
    
    print("=== –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π ===\n")
    
    print("üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:")
    for i, cat in enumerate(categories, 1):
        tokens = tokenize(cat)
        print(f"{i}. '{cat}' -> —Ç–æ–∫–µ–Ω—ã: {tokens}")
    print()
    
    print("üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:")
    
    for search in searches:
        print(f"\nüîé –ü–æ–∏—Å–∫: '{search}'")
        search_tokens = tokenize(search)
        print(f"   –¢–æ–∫–µ–Ω—ã –ø–æ–∏—Å–∫–∞: {search_tokens}")
        
        found = None
        
        # –ú–µ—Ç–æ–¥ 1: –ø–æ–∏—Å–∫ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤
        for cat in categories:
            cat_tokens = tokenize(cat)
            if search_tokens.issubset(cat_tokens):
                found = cat
                print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ (—Ç–æ–∫–µ–Ω—ã): '{cat}'")
                break
        
        if not found:
            # –ú–µ—Ç–æ–¥ 2: —Ç–æ—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
            search_norm = _normalize_letters(search.strip())
            for cat in categories:
                cleaned = _normalize_letters(re.sub(r"^[^\w–∞-—è–ê-–Ø—ë–Å]+", "", cat).strip())
                if cleaned == search_norm:
                    found = cat
                    print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ (—Ç–æ—á–Ω–æ–µ): '{cat}' ('{cleaned}' == '{search_norm}')")
                    break
        
        if not found:
            print(f"   ‚ùå –ù–ï –ù–ê–ô–î–ï–ù–û")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—á–µ–º—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
            print(f"      –ü—Ä–∏—á–∏–Ω–∞: —Ç–æ–∫–µ–Ω—ã '{search_tokens}' –Ω–µ —è–≤–ª—è—é—Ç—Å—è –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –Ω–∏ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏")
            print(f"      –ò –Ω–µ—Ç —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è '{_normalize_letters(search)}'")

if __name__ == "__main__":
    test_category_matching()