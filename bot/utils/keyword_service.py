"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å keywords —Ä–∞—Å—Ö–æ–¥–æ–≤ –∏ –¥–æ—Ö–æ–¥–æ–≤.
–ï–¥–∏–Ω—ã–π –∫–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –ø–æ–∏—Å–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ keywords.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π –ø—Ä–∏ keyword matching:
- –í–º–µ—Å—Ç–æ –ø–æ–∏—Å–∫–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ ("—Ç–µ—Å—Ç" –≤ "–≤ —Ç–µ—Å—Ç–µ") –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö —Ñ—Ä–∞–∑
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è 2 —É—Ä–æ–≤–Ω—è –ø—Ä–æ–≤–µ—Ä–∫–∏: —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ + —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–∞—á–∞–ª–∞ —Ñ—Ä–∞–∑—ã
- –ï–¥–∏–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä–∞—Å—Ö–æ–¥–æ–≤ –∏ –¥–æ—Ö–æ–¥–æ–≤
"""
import re
import logging
from typing import Tuple, Optional

logger = logging.getLogger(__name__)


def normalize_keyword_text(text: str) -> str:
    """
    –ï–¥–∏–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è keywords.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ò –ø—Ä–∏ –ø–æ–∏—Å–∫–µ.

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (description –∏–ª–∏ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å)

    Returns:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (lowercase, trim, –±–µ–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏/—ç–º–æ–¥–∑–∏)

    Examples:
        >>> normalize_keyword_text("  –°–æ—Å–∏—Å–∫–∞ –≤ –¢–ï–°–¢–ï –∏ —á–∞–π  ")
        "—Å–æ—Å–∏—Å–∫–∞ –≤ —Ç–µ—Å—Ç–µ –∏ —á–∞–π"
        >>> normalize_keyword_text("–ö–æ—Ñ–µ, —á–∞–π")
        "–∫–æ—Ñ–µ —á–∞–π"
        >>> normalize_keyword_text("üçï –ü–∏—Ü—Ü–∞!")
        "–ø–∏—Ü—Ü–∞"
    """
    if not text:
        return ""

    # 1. Lowercase
    normalized = text.lower()

    # 2. –£–¥–∞–ª—è–µ–º —ç–º–æ–¥–∑–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—É—é —É—Ç–∏–ª–∏—Ç—É –∏–∑ –ø—Ä–æ–µ–∫—Ç–∞)
    try:
        from bot.utils.emoji_utils import EMOJI_PREFIX_RE
        # EMOJI_PREFIX_RE —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Å—Ç—Ä–æ–∫–∏, –ø–æ—ç—Ç–æ–º—É —É–¥–∞–ª—è–µ–º –≤—Å–µ —ç–º–æ–¥–∑–∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ
        emoji_pattern = re.compile(
            r'[\U0001F000-\U0001F9FF'  # Emoticons, symbols, pictographs
            r'\U00002600-\U000027BF'    # Miscellaneous Symbols
            r'\U0001F300-\U0001F64F'    # Miscellaneous Symbols and Pictographs
            r'\U0001F680-\U0001F6FF'    # Transport and Map Symbols
            r'\u2600-\u27BF'            # Miscellaneous Symbols (compact)
            r'\u2300-\u23FF'            # Miscellaneous Technical
            r'\u2B00-\u2BFF'            # Miscellaneous Symbols and Arrows
            r'\u26A0-\u26FF'            # Miscellaneous Symbols
            r'\uFE00-\uFE0F'            # Variation Selectors
            r'\U000E0100-\U000E01EF'    # Variation Selectors Supplement
            r'\u200d'                   # Zero-Width Joiner (ZWJ)
            r'\ufe0f'                   # Variation Selector-16
            r']+',
            flags=re.UNICODE
        )
        normalized = emoji_pattern.sub('', normalized)
    except ImportError:
        # Fallback: –ø—Ä–æ—Å—Ç–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —ç–º–æ–¥–∑–∏ —á–µ—Ä–µ–∑ –±–∞–∑–æ–≤—ã–π regex
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # —ç–º–æ—Ü–∏–∏
            "\U0001F300-\U0001F5FF"  # —Å–∏–º–≤–æ–ª—ã
            "\U0001F680-\U0001F6FF"  # —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç
            "\U0001F1E0-\U0001F1FF"  # —Ñ–ª–∞–≥–∏
            "\U00002700-\U000027BF"  # —Ä–∞–∑–Ω–æ–µ
            "]+",
            flags=re.UNICODE
        )
        normalized = emoji_pattern.sub('', normalized)

    # 3. –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é (–∫—Ä–æ–º–µ –¥–µ—Ñ–∏—Å–∞ –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤)
    # –û—Å—Ç–∞–≤–ª—è–µ–º –±—É–∫–≤—ã (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞ + –ª–∞—Ç–∏–Ω–∏—Ü–∞), —Ü–∏—Ñ—Ä—ã, –ø—Ä–æ–±–µ–ª—ã, –¥–µ—Ñ–∏—Å
    normalized = re.sub(r'[^\w\s\-]', ' ', normalized, flags=re.UNICODE)
    # –£–¥–∞–ª—è–µ–º –¥–µ—Ñ–∏—Å—ã –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö —Å–ª–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏)
    normalized = re.sub(r'(?<!\w)-|-(?!\w)', ' ', normalized)

    # 4. Trim + —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤
    normalized = ' '.join(normalized.split())

    return normalized


def ensure_unique_keyword(
    profile,  # Profile
    category,  # Union[ExpenseCategory, IncomeCategory]
    word: str,
    is_income: bool = False
) -> Tuple[Optional[object], bool, int]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ keywords.
    –†–∞–±–æ—Ç–∞–µ—Ç –∏ –¥–ª—è —Ä–∞—Å—Ö–æ–¥–æ–≤ (CategoryKeyword), –∏ –¥–ª—è –¥–æ—Ö–æ–¥–æ–≤ (IncomeCategoryKeyword).

    –í–ê–ñ–ù–û: –û–¥–Ω–æ —Å–ª–æ–≤–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –≤ –û–î–ù–û–ô –∫–∞—Ç–µ–≥–æ—Ä–∏–∏!

    –ê–ª–≥–æ—Ä–∏—Ç–º:
    1. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–ª–æ–≤–æ
    2. –£–î–ê–õ–Ø–ï–¢ —Å–ª–æ–≤–æ –∏–∑ –í–°–ï–• –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—Ä–∞—Å—Ö–æ–¥–æ–≤ –∏–ª–∏ –¥–æ—Ö–æ–¥–æ–≤)
    3. –°–æ–∑–¥–∞–µ—Ç/–ø–æ–ª—É—á–∞–µ—Ç —Å–ª–æ–≤–æ –≤ —Ü–µ–ª–µ–≤–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    4. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (keyword, created, removed_count)

    Args:
        profile: –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        category: –¶–µ–ª–µ–≤–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è (ExpenseCategory –∏–ª–∏ IncomeCategory)
        word: –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
        is_income: True –¥–ª—è –¥–æ—Ö–æ–¥–æ–≤, False –¥–ª—è —Ä–∞—Å—Ö–æ–¥–æ–≤

    Returns:
        (keyword, created, removed_count):
            - keyword: –æ–±—ä–µ–∫—Ç CategoryKeyword –∏–ª–∏ IncomeCategoryKeyword (–∏–ª–∏ None –µ—Å–ª–∏ —Å–ª–æ–≤–æ –∫–æ—Ä–æ—á–µ 3 —Å–∏–º–≤–æ–ª–æ–≤)
            - created: True –µ—Å–ª–∏ —Å–ª–æ–≤–æ —Å–æ–∑–¥–∞–Ω–æ, False –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–ª–æ
            - removed_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

    Note:
        –ü–æ–ª–µ 'language' –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è —Ç.–∫. –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.
        CategoryKeyword –∏–º–µ–µ—Ç —ç—Ç–æ –ø–æ–ª–µ –≤ —Å—Ö–µ–º–µ, –Ω–æ –∫–æ–¥ –µ–≥–æ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç.
        IncomeCategoryKeyword –≤–æ–æ–±—â–µ –Ω–µ –∏–º–µ–µ—Ç –ø–æ–ª—è language.
    """
    from expenses.models import CategoryKeyword, IncomeCategoryKeyword

    # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
    KeywordModel = IncomeCategoryKeyword if is_income else CategoryKeyword

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–ª–æ–≤–æ
    normalized_word = normalize_keyword_text(word)

    if not normalized_word or len(normalized_word) < 3:
        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π –æ–±—ä–µ–∫—Ç –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (–Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î)
        logger.debug(f"Keyword too short: '{normalized_word}', skipping")
        return None, False, 0

    # –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï max_length=100 (CategoryKeyword.keyword / IncomeCategoryKeyword.keyword)
    # –û–±—Ä–µ–∑–∞–µ–º –ø–æ —Å–ª–æ–≤–∞–º, —á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—Ç—å —Å–ª–æ–≤–∞ –ø–æ—Å–µ—Ä–µ–¥–∏–Ω–µ
    if len(normalized_word) > 100:
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ 100 —Å–∏–º–≤–æ–ª–æ–≤
        truncated = normalized_word[:100]
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–æ–±–µ–ª, —á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—Ç—å —Å–ª–æ–≤–æ
        last_space = truncated.rfind(' ')
        if last_space > 0:
            normalized_word = truncated[:last_space].strip()
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–æ–±–µ–ª–æ–≤ - –æ–±—Ä–µ–∑–∞–µ–º –∂–µ—Å—Ç–∫–æ
            normalized_word = truncated.strip()

        logger.debug(
            f"Keyword truncated from {len(word)} to {len(normalized_word)} chars: "
            f"'{normalized_word}...'"
        )

    # –°–¢–†–û–ì–ê–Ø –£–ù–ò–ö–ê–õ–¨–ù–û–°–¢–¨: —É–¥–∞–ª—è–µ–º —Å–ª–æ–≤–æ –∏–∑ –í–°–ï–• –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    # –ë–ï–ó —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —è–∑—ã–∫—É - —Ç.–∫. –ø–æ–ª–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ production –∫–æ–¥–µ
    deleted = KeywordModel.objects.filter(
        category__profile=profile,
        keyword=normalized_word
    ).delete()

    removed_count = deleted[0] if deleted else 0

    if removed_count > 0:
        logger.debug(
            f"Removed keyword '{normalized_word}' from {removed_count} "
            f"{'income' if is_income else 'expense'} categories to maintain uniqueness"
        )

    # –°–æ–∑–¥–∞–µ–º/–ø–æ–ª—É—á–∞–µ–º keyword –≤ —Ü–µ–ª–µ–≤–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    # –ë–ï–ó —É–∫–∞–∑–∞–Ω–∏—è —è–∑—ã–∫–∞ - –ø–æ–ª–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
    keyword, created = KeywordModel.objects.get_or_create(
        category=category,
        keyword=normalized_word,
        defaults={'usage_count': 0}
    )

    return keyword, created, removed_count


def match_keyword_in_text(
    keyword: str,
    text: str,
    min_words: int = 2,
    max_prefix_words: int = 3
) -> Tuple[bool, str]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ keyword —Å —Ç–µ–∫—Å—Ç–æ–º (3 —É—Ä–æ–≤–Ω—è).

    –£—Ä–æ–≤–Ω–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏:
    1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π —Ñ—Ä–∞–∑—ã
    2. –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–∞—á–∞–ª–∞ —Ñ—Ä–∞–∑—ã (–ø–µ—Ä–≤—ã–µ 2-3 —Å–ª–æ–≤–∞, –µ—Å–ª–∏ >= 2 —Å–ª–æ–≤)
    3. –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å–æ —Å–∫–ª–æ–Ω–µ–Ω–∏—è–º–∏ (–¥–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö keywords >= 4 —Å–∏–º–≤–æ–ª–æ–≤ —Å –ø–µ—Ä–≤—ã–º —Å–ª–æ–≤–æ–º —Ç–µ–∫—Å—Ç–∞)

    Args:
        keyword: –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π keyword (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π)
        text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–±—É–¥–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω)
        min_words: –ú–∏–Ω–∏–º—É–º —Å–ª–æ–≤ –¥–ª—è prefix matching (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2)
        max_prefix_words: –ú–∞–∫—Å–∏–º—É–º —Å–ª–æ–≤ –¥–ª—è prefix (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3)

    Returns:
        (matched, match_type):
            - matched: True –µ—Å–ª–∏ –µ—Å—Ç—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, False –∏–Ω–∞—á–µ
            - match_type: "exact", "prefix", "inflection", –∏–ª–∏ "none"

    Examples:
        >>> match_keyword_in_text("—Å–æ—Å–∏—Å–∫–∞ –≤ —Ç–µ—Å—Ç–µ –∏ —á–∞–π", "–°–æ—Å–∏—Å–∫–∞ –≤ —Ç–µ—Å—Ç–µ –∏ —á–∞–π 390")
        (True, "prefix")  # —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç "390" –≤ –∫–æ–Ω—Ü–µ, –ø–æ—ç—Ç–æ–º—É —ç—Ç–æ prefix —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        >>> match_keyword_in_text("—Å–æ—Å–∏—Å–∫–∞ –≤ —Ç–µ—Å—Ç–µ –∏ —á–∞–π", "—Å–æ—Å–∏—Å–∫–∞ –≤ —Ç–µ—Å—Ç–µ –∏ —á–∞–π")
        (True, "exact")  # –ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤
        >>> match_keyword_in_text("–∑–∞—Ä–ø–ª–∞—Ç–∞", "–ó–∞—Ä–ø–ª–∞—Ç—É –ø–µ—Ä–µ–≤–µ–ª–∏")
        (True, "inflection")  # —Å–∫–ª–æ–Ω–µ–Ω–∏–µ –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ keyword —Å –ø–µ—Ä–≤—ã–º —Å–ª–æ–≤–æ–º —Ç–µ–∫—Å—Ç–∞
        >>> match_keyword_in_text("–∑–∞—Ä–ø–ª–∞—Ç–∞", "–ó–∞—Ä–ø–ª–∞—Ç–∞ –æ—Ç –∫–æ–º–ø–∞–Ω–∏–∏")
        (True, "inflection")  # –æ–¥–∏–Ω–æ—á–Ω—ã–π keyword –º–∞—Ç—á–∏—Ç –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ —Ñ—Ä–∞–∑—ã
        >>> match_keyword_in_text("–¥–æ–ª–≥ –∑–∞ —Ç–µ—Å—Ç", "–¢–µ—Å—Ç 500")
        (False, "none")  # –Ω–∞—á–∞–ª–æ –ù–ï —Å–æ–≤–ø–∞–¥–∞–µ—Ç
    """
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ–±–∞ —Ç–µ–∫—Å—Ç–∞
    normalized_keyword = normalize_keyword_text(keyword)
    normalized_text = normalize_keyword_text(text)

    if not normalized_keyword or not normalized_text:
        return False, "none"

    # –ó–ê–©–ò–¢–ê: –ú–∏–Ω–∏–º—É–º 3 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è keyword (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç "–≤", "–Ω–∞")
    if len(normalized_keyword) < 3:
        return False, "none"

    # –£–†–û–í–ï–ù–¨ 1: –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π —Ñ—Ä–∞–∑—ã
    if normalized_text == normalized_keyword:
        return True, "exact"

    text_words = normalized_text.split()
    keyword_words = normalized_keyword.split()

    # –£–†–û–í–ï–ù–¨ 2: –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–∞—á–∞–ª–∞ —Ñ—Ä–∞–∑—ã (–ø–µ—Ä–≤—ã–µ 2-3 —Å–ª–æ–≤–∞)
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ >= min_words —Å–ª–æ–≤
    if len(text_words) >= min_words and len(keyword_words) >= min_words:
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ N —Å–ª–æ–≤ (2-3)
        prefix_length = min(max_prefix_words, len(text_words), len(keyword_words))
        text_prefix = ' '.join(text_words[:prefix_length])
        keyword_prefix = ' '.join(keyword_words[:prefix_length])

        # –ó–∞—â–∏—Ç–∞ –æ—Ç –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤ (< 3 —Å–∏–º–≤–æ–ª–∞)
        if len(text_prefix) >= 3 and text_prefix == keyword_prefix:
            return True, "prefix"

    # –£–†–û–í–ï–ù–¨ 3: –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å–æ —Å–∫–ª–æ–Ω–µ–Ω–∏—è–º–∏ (–¥–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö keywords)
    # –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¢–û–õ–¨–ö–û –µ—Å–ª–∏ keyword = –æ–¥–Ω–æ —Å–ª–æ–≤–æ >= 4 —Å–∏–º–≤–æ–ª–æ–≤
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∫–ª–æ–Ω–µ–Ω–∏–µ —Å –õ–Æ–ë–´–ú —Å–ª–æ–≤–æ–º —Ç–µ–∫—Å—Ç–∞ >= 4 —Å–∏–º–≤–æ–ª–æ–≤
    # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç "–∑–∞—Ä–ø–ª–∞—Ç–∞" –º–∞—Ç—á–∏—Ç—å "–ø–µ—Ä–µ–≤–µ–ª–∏ –∑–∞—Ä–ø–ª–∞—Ç—É" –∏–ª–∏ "–∑–∞—Ä–ø–ª–∞—Ç–∞ –æ—Ç –∫–æ–º–ø–∞–Ω–∏–∏"
    if len(keyword_words) == 1 and len(normalized_keyword) >= 4:
        for text_word in text_words:
            if len(text_word) >= 4:
                # –ë–µ—Ä–µ–º –û–°–ù–û–í–£ —Å–ª–æ–≤–∞ (–±–µ–∑ –æ–∫–æ–Ω—á–∞–Ω–∏—è) - —É–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 —Å–∏–º–≤–æ–ª–∞ –æ—Ç –º–µ–Ω—å—à–µ–≥–æ —Å–ª–æ–≤–∞
                min_len = min(len(normalized_keyword), len(text_word))
                # –û—Å–Ω–æ–≤–∞ = –º–∏–Ω–∏–º—É–º –º–∏–Ω—É—Å 2 —Å–∏–º–≤–æ–ª–∞ (–æ–∫–æ–Ω—á–∞–Ω–∏–µ), –Ω–æ –Ω–µ –º–µ–Ω—å—à–µ 4
                stem_len = max(4, min_len - 2)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ—Å–Ω–æ–≤—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç
                keyword_stem = normalized_keyword[:stem_len]
                text_stem = text_word[:stem_len]

                if keyword_stem == text_stem:
                    # –†–∞–∑–Ω–∏—Ü–∞ –≤ –¥–ª–∏–Ω–µ –Ω–µ –±–æ–ª—å—à–µ 2 —Å–∏–º–≤–æ–ª–æ–≤ (–æ–∫–æ–Ω—á–∞–Ω–∏–µ)
                    diff = abs(len(normalized_keyword) - len(text_word))
                    if diff <= 2:
                        return True, "inflection"

    return False, "none"
