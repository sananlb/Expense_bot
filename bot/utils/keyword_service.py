"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å keywords —Ä–∞—Å—Ö–æ–¥–æ–≤ –∏ –¥–æ—Ö–æ–¥–æ–≤.
–ï–¥–∏–Ω—ã–π –∫–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –ø–æ–∏—Å–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ keywords.

–õ–æ–≥–∏–∫–∞ –ø–æ–∏—Å–∫–∞:
- –£—Ä–æ–≤–µ–Ω—å 1 (Exact): –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π —Ñ—Ä–∞–∑—ã (¬±1 –±—É–∫–≤–∞ –Ω–∞ –≤—Å—é —Ñ—Ä–∞–∑—É)
- –£—Ä–æ–≤–µ–Ω—å 2 (Word): –ü–æ–∏—Å–∫ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ —Ç–µ–∫—Å—Ç–∞ –≤ keywords (¬±1 –±—É–∫–≤–∞ –Ω–∞ —Å–ª–æ–≤–æ)

–ü—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏ –ø–æ–∏—Å–∫–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —É–¥–∞–ª—è—é—Ç—Å—è –º—É—Å–æ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ (STOP_WORDS).
"""
import re
import logging
from typing import Tuple, Optional, Set

logger = logging.getLogger(__name__)


# =============================================================================
# STOP_WORDS - –º—É—Å–æ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞, —É–¥–∞–ª—è–µ–º—ã–µ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏ –ø–æ–∏—Å–∫–µ keywords
# =============================================================================

STOP_WORDS: Set[str] = {
    # === –ü—Ä–µ–¥–ª–æ–≥–∏ –∏ —Å–æ—é–∑—ã (RU) ===
    '–∏', '–≤', '–Ω–∞', '—Å', '–∑–∞', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑',
    '–∏–ª–∏', '–Ω–æ', '–∞', '–∫', '—É', '–æ', '–æ–±', '–ø–æ–¥', '–Ω–∞–¥',
    '–ø—Ä–∏', '–±–µ–∑', '–º–µ–∂–¥—É', '—á–µ—Ä–µ–∑', '–æ–∫–æ–ª–æ', '—Ä–∞–¥–∏', '–≤–º–µ—Å—Ç–æ',

    # === –ü—Ä–µ–¥–ª–æ–≥–∏ –∏ —Å–æ—é–∑—ã (EN) ===
    'and', 'or', 'to', 'for', 'from', 'with', 'at', 'by', 'in', 'on',
    'the', 'a', 'an', 'of', 'as', 'is', 'it', 'if', 'so',

    # === –ì–ª–∞–≥–æ–ª—ã –¥–µ–π—Å—Ç–≤–∏—è (RU) ===
    '–∫—É–ø–∏–ª', '–∫—É–ø–∏–ª–∞', '–∫—É–ø–∏–ª–∏', '–∫—É–ø–∏—Ç—å',
    '–≤–∑—è–ª', '–≤–∑—è–ª–∞', '–≤–∑—è–ª–∏', '–≤–∑—è—Ç—å',
    '–ø–æ—Ç—Ä–∞—Ç–∏–ª', '–ø–æ—Ç—Ä–∞—Ç–∏–ª–∞', '–ø–æ—Ç—Ä–∞—Ç–∏–ª–∏', '–ø–æ—Ç—Ä–∞—Ç–∏—Ç—å',
    '–æ–ø–ª–∞—Ç–∏–ª', '–æ–ø–ª–∞—Ç–∏–ª–∞', '–æ–ø–ª–∞—Ç–∏–ª–∏', '–æ–ø–ª–∞—Ç–∏—Ç—å',
    '–∑–∞–ø–ª–∞—Ç–∏–ª', '–∑–∞–ø–ª–∞—Ç–∏–ª–∞', '–∑–∞–ø–ª–∞—Ç–∏–ª–∏', '–∑–∞–ø–ª–∞—Ç–∏—Ç—å',
    '–∑–∞–∫–∞–∑–∞–ª', '–∑–∞–∫–∞–∑–∞–ª–∞', '–∑–∞–∫–∞–∑–∞–ª–∏', '–∑–∞–∫–∞–∑–∞—Ç—å',
    '–ø—Ä–∏–æ–±—Ä–µ–ª', '–ø—Ä–∏–æ–±—Ä–µ–ª–∞', '–ø—Ä–∏–æ–±—Ä–µ–ª–∏', '–ø—Ä–∏–æ–±—Ä–µ—Å—Ç–∏',
    '—Å—ä–µ–ª', '—Å—ä–µ–ª–∞', '—Å—ä–µ–ª–∏', '—Å—ä–µ—Å—Ç—å',
    '–≤—ã–ø–∏–ª', '–≤—ã–ø–∏–ª–∞', '–≤—ã–ø–∏–ª–∏', '–≤—ã–ø–∏—Ç—å',
    '—Å—Ö–æ–¥–∏–ª', '—Å—Ö–æ–¥–∏–ª–∞', '—Å—Ö–æ–¥–∏–ª–∏', '—Å—Ö–æ–¥–∏—Ç—å',
    '–æ—Ç–¥–∞–ª', '–æ—Ç–¥–∞–ª–∞', '–æ—Ç–¥–∞–ª–∏', '–æ—Ç–¥–∞—Ç—å',
    '–≤–Ω–µ—Å', '–≤–Ω–µ—Å–ª–∞', '–≤–Ω–µ—Å–ª–∏', '–≤–Ω–µ—Å—Ç–∏',
    '–ø–µ—Ä–µ–≤–µ–ª', '–ø–µ—Ä–µ–≤–µ–ª–∞', '–ø–µ—Ä–µ–≤–µ–ª–∏', '–ø–µ—Ä–µ–≤–µ—Å—Ç–∏',
    '–æ—Ç–ø—Ä–∞–≤–∏–ª', '–æ—Ç–ø—Ä–∞–≤–∏–ª–∞', '–æ—Ç–ø—Ä–∞–≤–∏–ª–∏', '–æ—Ç–ø—Ä–∞–≤–∏—Ç—å',
    '–ø–æ–ª–æ–∂–∏–ª', '–ø–æ–ª–æ–∂–∏–ª–∞', '–ø–æ–ª–æ–∂–∏–ª–∏', '–ø–æ–ª–æ–∂–∏—Ç—å',
    '—Å–Ω—è–ª', '—Å–Ω—è–ª–∞', '—Å–Ω—è–ª–∏', '—Å–Ω—è—Ç—å',
    '–ø–æ–ª—É—á–∏–ª', '–ø–æ–ª—É—á–∏–ª–∞', '–ø–æ–ª—É—á–∏–ª–∏', '–ø–æ–ª—É—á–∏—Ç—å',

    # === –ì–ª–∞–≥–æ–ª—ã –¥–µ–π—Å—Ç–≤–∏—è (EN) ===
    'bought', 'buy', 'paid', 'pay', 'spent', 'spend',
    'got', 'get', 'took', 'take', 'ordered', 'order',
    'had', 'have', 'made', 'make',

    # === –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (RU) ===
    '–≤—á–µ—Ä–∞', '—Å–µ–≥–æ–¥–Ω—è', '–∑–∞–≤—Ç—Ä–∞', '—É—Ç—Ä–æ–º', '–≤–µ—á–µ—Ä–æ–º', '–¥–Ω–µ–º', '–Ω–æ—á—å—é',
    '–ø–æ–∑–∞–≤—á–µ—Ä–∞', '–ø–æ—Å–ª–µ–∑–∞–≤—Ç—Ä–∞', '–Ω–µ–¥–∞–≤–Ω–æ', '–¥–∞–≤–Ω–æ', '–ø–æ—Å–ª–µ', '–ø–µ—Ä–µ–¥', '–¥–æ',

    # === –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (EN) ===
    'yesterday', 'today', 'tomorrow', 'morning', 'evening', 'night',
    'recently', 'ago',

    # === –ú–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è (RU) ===
    '—è', '–º–Ω–µ', '–º–Ω–æ–π', '–º–æ–π', '–º–æ—è', '–º–æ–µ', '–º–æ–∏',
    '—Å–µ–±–µ', '—Å–µ–±—è', '—Å–æ–±–æ–π',
    '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–æ', '–æ–Ω–∏', '–µ–º—É', '–µ–π', '–∏–º',
    '–º—ã', '–Ω–∞–º', '–Ω–∞—Å', '–Ω–∞—à', '–Ω–∞—à–∞', '–Ω–∞—à–∏',

    # === –ú–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è (EN) ===
    'i', 'me', 'my', 'myself', 'mine',
    'you', 'your', 'yourself', 'yours',
    'he', 'she', 'it', 'they', 'we', 'us', 'our',

    # === –í–∞–ª—é—Ç—ã - –∫–æ–¥—ã (–≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –≤ –±–æ—Ç–µ) ===
    'rub', 'usd', 'eur', 'gbp', 'cny', 'chf', 'inr', 'try', 'jpy',
    'ars', 'cop', 'pen', 'clp', 'mxn', 'brl', 'aed',
    'kzt', 'uah', 'byn', 'byr', 'uzs', 'amd', 'azn', 'kgs', 'tjs', 'tmt', 'mdl', 'gel',

    # === –í–∞–ª—é—Ç—ã - –Ω–∞–∑–≤–∞–Ω–∏—è (RU) ===
    '—Ä—É–±–ª—å', '—Ä—É–±–ª—è', '—Ä—É–±–ª–µ–π', '—Ä—É–±',
    '–¥–æ–ª–ª–∞—Ä', '–¥–æ–ª–ª–∞—Ä–∞', '–¥–æ–ª–ª–∞—Ä–æ–≤', '–¥–æ–ª–ª',
    '–µ–≤—Ä–æ',
    '—Ñ—É–Ω—Ç', '—Ñ—É–Ω—Ç–∞', '—Ñ—É–Ω—Ç–æ–≤',
    '—é–∞–Ω—å', '—é–∞–Ω—è', '—é–∞–Ω–µ–π',
    '—Ñ—Ä–∞–Ω–∫', '—Ñ—Ä–∞–Ω–∫–∞', '—Ñ—Ä–∞–Ω–∫–æ–≤',
    '—Ä—É–ø–∏—è', '—Ä—É–ø–∏–∏', '—Ä—É–ø–∏–π', '—Ä—É–ø–∏',
    '–ª–∏—Ä–∞', '–ª–∏—Ä—ã', '–ª–∏—Ä',
    '–π–µ–Ω–∞', '–π–µ–Ω—ã', '–π–µ–Ω', '–∏–µ–Ω–∞', '–∏–µ–Ω—ã', '–∏–µ–Ω',
    '—Ç–µ–Ω–≥–µ', '—Ç–µ–Ω—å–≥–µ', '—Ç–Ω–≥', '—Ç–≥',
    '–≥—Ä–∏–≤–Ω–∞', '–≥—Ä–∏–≤–Ω—ã', '–≥—Ä–∏–≤–µ–Ω', '–≥—Ä–Ω',
    '—Å—É–º', '—Å—É–º–∞', '—Å—É–º–æ–≤',
    '–¥—Ä–∞–º', '–¥—Ä–∞–º–∞', '–¥—Ä–∞–º–æ–≤',
    '–º–∞–Ω–∞—Ç', '–º–∞–Ω–∞—Ç–∞', '–º–∞–Ω–∞—Ç–æ–≤', '–º–∞–Ω–∞—Ç—ã',
    # '—Å–æ–º' —É–±—Ä–∞–Ω ‚Äî –∫–æ–Ω—Ñ–ª–∏–∫—Ç —Å —Ä—ã–±–æ–π "—Å–æ–º"
    '—Å–æ–º–æ–Ω–∏',
    '–ª–µ–π', '–ª–µ—è', '–ª–µ–µ–≤', '–ª–µ–∏',
    '–ª–∞—Ä–∏',
    '–¥–∏—Ä—Ö–∞–º', '–¥–∏—Ä—Ö–∞–º–∞', '–¥–∏—Ä—Ö–∞–º–æ–≤', '–¥–∏—Ä—Ö–∞–º—ã', '–¥–∏—Ä—Ö–∞–º–µ',
    '—Ä–µ–∞–ª', '—Ä–µ–∞–ª–∞', '—Ä–µ–∞–ª–æ–≤', '—Ä–µ–∞–ª—ã',
    '–ø–µ—Å–æ',
    # '—Å–æ–ª—å' —É–±—Ä–∞–Ω ‚Äî –∫–æ–Ω—Ñ–ª–∏–∫—Ç —Å –ø—Ä–æ–¥—É–∫—Ç–æ–º "—Å–æ–ª—å"
    '—Å–æ–ª–µ–π',
    # –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω (–¥–ª—è ¬´500 –º–µ–∫—Å–∏–∫–∞–Ω—Å–∫–∏—Ö¬ª –∏ —Ç.–ø.)
    '–∞—Ä–≥–µ–Ω—Ç–∏–Ω—Å–∫–∏—Ö', '–∞—Ä–≥–µ–Ω—Ç–∏–Ω—Å–∫–æ–µ', '–∞—Ä–≥–µ–Ω—Ç–∏–Ω—Å–∫–∏–π',
    '–∫–æ–ª—É–º–±–∏–π—Å–∫–∏—Ö', '–∫–æ–ª—É–º–±–∏–π—Å–∫–æ–µ', '–∫–æ–ª—É–º–±–∏–π—Å–∫–∏–π',
    '–ø–µ—Ä—É–∞–Ω—Å–∫–∏—Ö', '–ø–µ—Ä—É–∞–Ω—Å–∫–æ–µ', '–ø–µ—Ä—É–∞–Ω—Å–∫–∏–π',
    '—á–∏–ª–∏–π—Å–∫–∏—Ö', '—á–∏–ª–∏–π—Å–∫–æ–µ', '—á–∏–ª–∏–π—Å–∫–∏–π',
    '–º–µ–∫—Å–∏–∫–∞–Ω—Å–∫–∏—Ö', '–º–µ–∫—Å–∏–∫–∞–Ω—Å–∫–æ–µ', '–º–µ–∫—Å–∏–∫–∞–Ω—Å–∫–∏–π',
    '–±—Ä–∞–∑–∏–ª—å—Å–∫–∏—Ö', '–±—Ä–∞–∑–∏–ª—å—Å–∫–æ–µ', '–±—Ä–∞–∑–∏–ª—å—Å–∫–∏–π',

    # === –í–∞–ª—é—Ç—ã - –Ω–∞–∑–≤–∞–Ω–∏—è (EN) ===
    'dollar', 'dollars', 'buck', 'bucks',
    'euro', 'euros',
    'pound', 'pounds', 'sterling',
    'yuan', 'renminbi', 'rmb',
    'franc', 'francs',
    'rupee', 'rupees',
    'lira', 'liras',
    'yen',
    'peso', 'pesos',
    'real', 'reals',
    'tenge',
    'hryvnia', 'hryvnya',
    # 'som', 'soum' —É–±—Ä–∞–Ω—ã ‚Äî –∫–æ–Ω—Ñ–ª–∏–∫—Ç —Å —Ä—ã–±–æ–π
    'somoni',
    'manat',
    'dram',
    'lari',
    'lei',
    'dirham', 'dirhams',
    'sol',

    # === –ß–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –µ–¥–∏–Ω–∏—Ü—ã ===
    '—Ç—ã—Å', '—Ç—ã—Å—è—á', '—Ç—ã—Å—è—á–∏', '—Ç—ã—Å—è—á—É',
    '–º–ª–Ω', '–º–∏–ª–ª–∏–æ–Ω', '–º–∏–ª–ª–∏–æ–Ω–∞', '–º–∏–ª–ª–∏–æ–Ω–æ–≤',
    'thousand', 'million', 'billion',
    '—à—Ç', '—à—Ç—É–∫', '—à—Ç—É–∫–∏',
}


def normalize_keyword_text(text: str) -> str:
    """
    –ë–∞–∑–æ–≤–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è keywords.
    –ù–ï —É–¥–∞–ª—è–µ—Ç stop words - —ç—Ç–æ –¥–µ–ª–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è.

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (description –∏–ª–∏ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å)

    Returns:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (lowercase, trim, –±–µ–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏/—ç–º–æ–¥–∑–∏)

    Examples:
        >>> normalize_keyword_text("  –°–æ—Å–∏—Å–∫–∞ –≤ –¢–ï–°–¢–ï –∏ —á–∞–π  ")
        "—Å–æ—Å–∏—Å–∫–∞ –≤ —Ç–µ—Å—Ç–µ –∏ —á–∞–π"
        >>> normalize_keyword_text("–ö–æ—Ñ–µ, —á–∞–π")
        "–∫–æ—Ñ–µ —á–∞–π"
        >>> normalize_keyword_text("üçï –ü–∏—Ü—Ü–∞!")
        "–ø–∏—Ü—Ü–∞"
    """
    if not text:
        return ""

    # 1. Lowercase
    normalized = text.lower()

    # 2. –£–¥–∞–ª—è–µ–º —ç–º–æ–¥–∑–∏
    emoji_pattern = re.compile(
        r'[\U0001F000-\U0001F9FF'  # Emoticons, symbols, pictographs
        r'\U00002600-\U000027BF'    # Miscellaneous Symbols
        r'\U0001F300-\U0001F64F'    # Miscellaneous Symbols and Pictographs
        r'\U0001F680-\U0001F6FF'    # Transport and Map Symbols
        r'\u2600-\u27BF'            # Miscellaneous Symbols (compact)
        r'\u2300-\u23FF'            # Miscellaneous Technical
        r'\u2B00-\u2BFF'            # Miscellaneous Symbols and Arrows
        r'\u26A0-\u26FF'            # Miscellaneous Symbols
        r'\uFE00-\uFE0F'            # Variation Selectors
        r'\U000E0100-\U000E01EF'    # Variation Selectors Supplement
        r'\u200d'                   # Zero-Width Joiner (ZWJ)
        r'\ufe0f'                   # Variation Selector-16
        r']+',
        flags=re.UNICODE
    )
    normalized = emoji_pattern.sub('', normalized)

    # 3. –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –≤–∞–ª—é—Ç–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    # –û—Å—Ç–∞–≤–ª—è–µ–º –±—É–∫–≤—ã (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞ + –ª–∞—Ç–∏–Ω–∏—Ü–∞), —Ü–∏—Ñ—Ä—ã, –ø—Ä–æ–±–µ–ª—ã, –¥–µ—Ñ–∏—Å –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤
    # –ß–∏—Å–ª–∞ –ù–ï —É–¥–∞–ª—è–µ–º ‚Äî –æ–Ω–∏ –º–æ–≥—É—Ç –±—ã—Ç—å —á–∞—Å—Ç—å—é –Ω–∞–∑–≤–∞–Ω–∏—è ("–±–µ–Ω–∑–∏–Ω 95", "iPhone 15")
    normalized = re.sub(r'[^\w\s\-]', ' ', normalized, flags=re.UNICODE)
    # –£–¥–∞–ª—è–µ–º –¥–µ—Ñ–∏—Å—ã –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö —Å–ª–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏)
    normalized = re.sub(r'(?<!\w)-|-(?!\w)', ' ', normalized)

    # 4. Trim + —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤
    normalized = ' '.join(normalized.split())

    return normalized


def remove_stop_words(text: str) -> str:
    """
    –£–¥–∞–ª—è–µ—Ç –º—É—Å–æ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞.

    Args:
        text: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (lowercase)

    Returns:
        –¢–µ–∫—Å—Ç –±–µ–∑ stop words

    Examples:
        >>> remove_stop_words("–∫—É–ø–∏–ª —Ç—Ä—É—Ö–ª—è–≤—ã–µ –∫–æ–Ω—Å–µ—Ä–≤—ã –≤—á–µ—Ä–∞")
        "—Ç—Ä—É—Ö–ª—è–≤—ã–µ –∫–æ–Ω—Å–µ—Ä–≤—ã"
        >>> remove_stop_words("—è –≤–∑—è–ª –∫–æ—Ñ–µ –∑–∞ 200 —Ä—É–±–ª–µ–π")
        "–∫–æ—Ñ–µ 200"
    """
    if not text:
        return ""

    words = text.split()
    filtered = [w for w in words if w not in STOP_WORDS and len(w) >= 2]
    return ' '.join(filtered)


def prepare_keyword_for_save(text: str) -> str:
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–∞–∫ keyword.
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç, —É–¥–∞–ª—è–µ—Ç stop words, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –§–†–ê–ó–£ –¶–ï–õ–ò–ö–û–ú.

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (description)

    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π keyword –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î.
        –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –µ—Å–ª–∏ —Ñ—Ä–∞–∑–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è (>4 —Å–ª–æ–≤ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏).

    Examples:
        >>> prepare_keyword_for_save("–í—á–µ—Ä–∞ –∫—É–ø–∏–ª —Ç—Ä—É—Ö–ª—è–≤—ã–µ –∫–æ–Ω—Å–µ—Ä–≤—ã")
        "—Ç—Ä—É—Ö–ª—è–≤—ã–µ –∫–æ–Ω—Å–µ—Ä–≤—ã"
        >>> prepare_keyword_for_save("–∫—É–ø–∏–ª –∫–æ—Ñ–µ –≤ —Å—Ç–∞—Ä–±–∞–∫—Å–µ")
        "–∫–æ—Ñ–µ —Å—Ç–∞—Ä–±–∞–∫—Å–µ"
        >>> prepare_keyword_for_save("–∏–≥—Ä–∞–ª —Å –¥—Ä—É–∑—å—è–º–∏ –≤ –ö–° –ø–æ—Å–ª–µ —à–∫–æ–ª—ã 300 —Ä—É–±–ª–µ–π")
        ""  # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è —Ñ—Ä–∞–∑–∞ ‚Äî –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
    """
    normalized = normalize_keyword_text(text)
    cleaned = remove_stop_words(normalized)

    # –î–ª–∏–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã (>4 —Å–ª–æ–≤) –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º ‚Äî –æ–Ω–∏ —Å–ª–∏—à–∫–æ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã
    # –∏ —Å–ª—É—á–∞–π–Ω—ã–µ —Å–ª–æ–≤–∞ ("–¥—Ä—É–∑—å—è–º–∏", "—à–∫–æ–ª—ã") —Å–ª–æ–º–∞—é—Ç –ª–æ–≥–∏–∫—É
    words = cleaned.split()
    if len(words) > 4:
        return ""

    return cleaned


def words_match_with_inflection(word1: str, word2: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–≤—É—Ö —Å–ª–æ–≤ —Å —É—á—ë—Ç–æ–º —Å–∫–ª–æ–Ω–µ–Ω–∏–π (¬±1 –±—É–∫–≤–∞).

    Args:
        word1: –ü–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ
        word2: –í—Ç–æ—Ä–æ–µ —Å–ª–æ–≤–æ

    Returns:
        True –µ—Å–ª–∏ —Å–ª–æ–≤–∞ —Å–æ–≤–ø–∞–¥–∞—é—Ç –∏–ª–∏ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –Ω–∞ 1 –±—É–∫–≤—É

    Examples:
        >>> words_match_with_inflection("–∑–∞—Ä–ø–ª–∞—Ç–∞", "–∑–∞—Ä–ø–ª–∞—Ç—É")
        True
        >>> words_match_with_inflection("–∫–æ—Ñ–µ", "–∫–æ—Ñ–µ")
        True
        >>> words_match_with_inflection("–∫–æ—Ñ–µ", "–∫–æ—Å–æ–π")
        False
        >>> words_match_with_inflection("–∫–±", "–∫–±")
        True  # 2-–±—É–∫–≤–µ–Ω–Ω—ã–µ: —Ç–æ–ª—å–∫–æ exact match
        >>> words_match_with_inflection("–∫–±", "–∫–≤")
        False  # 2-–±—É–∫–≤–µ–Ω–Ω—ã–µ: ¬±1 –±—É–∫–≤–∞ –ù–ï –¥–æ–ø—É—Å–∫–∞–µ—Ç—Å—è
    """
    if word1 == word2:
        return True

    # –ú–∏–Ω–∏–º—É–º 2 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    if len(word1) < 2 or len(word2) < 2:
        return False

    # –î–ª—è 2-–±—É–∫–≤–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤ ‚Äî —Ç–æ–ª—å–∫–æ exact match, –±–µ–∑ ¬±1 –±—É–∫–≤—ã
    # –≠—Ç–æ –∑–∞—â–∏—Ç–∞ –æ—Ç –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π: "–∫–±" –Ω–µ –¥–æ–ª–∂–µ–Ω –º–∞—Ç—á–∏—Ç—å "–∫–∞", "–∫–≤" –∏ —Ç.–¥.
    if len(word1) == 2 or len(word2) == 2:
        return False  # –£–∂–µ –ø—Ä–æ–≤–µ—Ä–∏–ª–∏ exact match –≤—ã—à–µ

    diff = abs(len(word1) - len(word2))

    # –ï—Å–ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –¥–ª–∏–Ω–µ > 1 —Å–∏–º–≤–æ–ª–∞ - –Ω–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    if diff > 1:
        return False

    # –ï—Å–ª–∏ –¥–ª–∏–Ω–∞ –æ–¥–∏–Ω–∞–∫–æ–≤–∞—è - —Å—á–∏—Ç–∞–µ–º –æ—Ç–ª–∏—á–∞—é—â–∏–µ—Å—è –±—É–∫–≤—ã
    if diff == 0:
        mismatches = sum(1 for a, b in zip(word1, word2) if a != b)
        return mismatches <= 1

    # –ï—Å–ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞ —Ä–æ–≤–Ω–æ 1 —Å–∏–º–≤–æ–ª
    shorter = word1 if len(word1) < len(word2) else word2
    longer = word2 if len(word1) < len(word2) else word1

    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –ø–æ–∑–∏—Ü–∏—è—Ö + 1 –∑–∞ –ª–∏—à–Ω–∏–π —Å–∏–º–≤–æ–ª
    mismatches = sum(1 for i, c in enumerate(shorter) if c != longer[i]) + 1
    return mismatches <= 1


def match_keyword_in_text(
    keyword: str,
    text: str,
) -> Tuple[bool, str]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ keyword —Å —Ç–µ–∫—Å—Ç–æ–º (2 —É—Ä–æ–≤–Ω—è).

    –í–ê–ñ–ù–û: Keyword –∏–∑ –ë–î —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å (—É–∂–µ –æ—á–∏—â–µ–Ω –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏).
           Text –æ—á–∏—â–∞–µ—Ç—Å—è –æ—Ç stop words –ø–µ—Ä–µ–¥ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º.

    –£—Ä–æ–≤–Ω–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏:
    1. Exact: –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –æ—á–∏—â–µ–Ω–Ω–æ–π —Ñ—Ä–∞–∑—ã (¬±1 –±—É–∫–≤–∞ –Ω–∞ –≤—Å—é —Ñ—Ä–∞–∑—É)
    2. Word: –ö–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏—â–µ–º –≤ keywords –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ (¬±1 –±—É–∫–≤–∞)

    Args:
        keyword: –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π keyword –∏–∑ –ë–î (—É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∏ –æ—á–∏—â–µ–Ω–Ω—ã–π)
        text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–±—É–¥–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω –∏ –æ—á–∏—â–µ–Ω –æ—Ç stop words)

    Returns:
        (matched, match_type):
            - matched: True –µ—Å–ª–∏ –µ—Å—Ç—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, False –∏–Ω–∞—á–µ
            - match_type: "exact", "word", –∏–ª–∏ "none"

    Examples:
        >>> match_keyword_in_text("—Ç—Ä—É—Ö–ª—è–≤—ã–µ –∫–æ–Ω—Å–µ—Ä–≤—ã", "–ö—É–ø–∏–ª —Ç—Ä—É—Ö–ª—è–≤—ã–µ –∫–æ–Ω—Å–µ—Ä–≤—ã –≤—á–µ—Ä–∞")
        (True, "exact")  # –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: "—Ç—Ä—É—Ö–ª—è–≤—ã–µ –∫–æ–Ω—Å–µ—Ä–≤—ã" == "—Ç—Ä—É—Ö–ª—è–≤—ã–µ –∫–æ–Ω—Å–µ—Ä–≤—ã"

        >>> match_keyword_in_text("–∫–æ–Ω—Å–µ—Ä–≤—ã", "–ö—É–ø–∏–ª —Ç—Ä—É—Ö–ª—è–≤—ã–µ –∫–æ–Ω—Å–µ—Ä–≤—ã –≤—á–µ—Ä–∞")
        (True, "word")  # keyword "–∫–æ–Ω—Å–µ—Ä–≤—ã" –Ω–∞–π–¥–µ–Ω –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ

        >>> match_keyword_in_text("–∑–∞—Ä–ø–ª–∞—Ç–∞", "–ú–Ω–µ –ø–µ—Ä–µ–≤–µ–ª–∏ –∑–∞—Ä–ø–ª–∞—Ç—É")
        (True, "word")  # "–∑–∞—Ä–ø–ª–∞—Ç–∞" ~= "–∑–∞—Ä–ø–ª–∞—Ç—É" (¬±1 –±—É–∫–≤–∞)

        >>> match_keyword_in_text("—Ç—Ä—É—Ö–ª—è–≤—ã–µ –∫–æ–Ω—Å–µ—Ä–≤—ã", "–¢—Ä—É—Ö–ª—è–≤—ã–µ –ø—Ä–æ—Å—Ä–æ—á–µ–Ω–Ω—ã–µ –∫–æ–Ω—Å–µ—Ä–≤—ã")
        (False, "none")  # —Ñ—Ä–∞–∑–∞ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç, –∞ "—Ç—Ä—É—Ö–ª—è–≤—ã–µ –∫–æ–Ω—Å–µ—Ä–≤—ã" != –æ–¥–Ω–æ–º—É —Å–ª–æ–≤—É
    """
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ –æ—á–∏—â–∞–µ–º keyword –æ—Ç stop words
    # (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω –Ω–µ –∏–∑ –ë–î, –Ω–∞–ø—Ä–∏–º–µ—Ä –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ "–ö–∞—Ñ–µ –∏ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã")
    normalized_keyword = normalize_keyword_text(keyword)
    cleaned_keyword = remove_stop_words(normalized_keyword)

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ –æ—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç stop words
    normalized_text = normalize_keyword_text(text)
    cleaned_text = remove_stop_words(normalized_text)

    if not cleaned_keyword or not cleaned_text:
        return False, "none"

    # –ó–ê–©–ò–¢–ê: –ú–∏–Ω–∏–º—É–º 2 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è keyword (–¥–ª—è –∫–±, –≤–≤, –∑–ø –∏ —Ç.–¥.)
    # 2-–±—É–∫–≤–µ–Ω–Ω—ã–µ keywords –º–∞—Ç—á–∞—Ç—Å—è —Ç–æ–ª—å–∫–æ exact, –±–µ–∑ ¬±1 –±—É–∫–≤—ã
    if len(cleaned_keyword) < 2:
        return False, "none"

    # =================================================================
    # –£–†–û–í–ï–ù–¨ 1: Exact - —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã (¬±1 –±—É–∫–≤–∞ –Ω–∞ –≤—Å—é —Ñ—Ä–∞–∑—É)
    # =================================================================
    if cleaned_text == cleaned_keyword:
        return True, "exact"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º ¬±1 –±—É–∫–≤—É –¥–ª—è –≤—Å–µ–π —Ñ—Ä–∞–∑—ã (–¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–∑)
    if len(cleaned_keyword) <= 15:  # –î–ª—è —Ñ—Ä–∞–∑ –¥–æ 15 —Å–∏–º–≤–æ–ª–æ–≤
        if words_match_with_inflection(cleaned_text, cleaned_keyword):
            return True, "exact"

    # =================================================================
    # –£–†–û–í–ï–ù–¨ 2: Word - –ø–æ–∏—Å–∫ keyword –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ
    # =================================================================
    # –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö keywords (–Ω–µ —Ñ—Ä–∞–∑)
    keyword_words = cleaned_keyword.split()

    if len(keyword_words) == 1:
        # Keyword - –æ–¥–Ω–æ —Å–ª–æ–≤–æ, –∏—â–µ–º –µ–≥–æ –≤ –ª—é–±–æ–º –º–µ—Å—Ç–µ —Ç–µ–∫—Å—Ç–∞
        text_words = cleaned_text.split()

        for text_word in text_words:
            if words_match_with_inflection(cleaned_keyword, text_word):
                return True, "word"

    return False, "none"


def ensure_unique_keyword(
    profile,  # Profile
    category,  # Union[ExpenseCategory, IncomeCategory]
    word: str,
    is_income: bool = False
) -> Tuple[Optional[object], bool, int]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ keywords.
    –†–∞–±–æ—Ç–∞–µ—Ç –∏ –¥–ª—è —Ä–∞—Å—Ö–æ–¥–æ–≤ (CategoryKeyword), –∏ –¥–ª—è –¥–æ—Ö–æ–¥–æ–≤ (IncomeCategoryKeyword).

    –í–ê–ñ–ù–û: –û–¥–Ω–æ —Å–ª–æ–≤–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –≤ –û–î–ù–û–ô –∫–∞—Ç–µ–≥–æ—Ä–∏–∏!

    –ê–ª–≥–æ—Ä–∏—Ç–º:
    1. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏ –æ—á–∏—â–∞–µ—Ç —Å–ª–æ–≤–æ –æ—Ç stop words
    2. –£–î–ê–õ–Ø–ï–¢ —Å–ª–æ–≤–æ –∏–∑ –í–°–ï–• –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—Ä–∞—Å—Ö–æ–¥–æ–≤ –∏–ª–∏ –¥–æ—Ö–æ–¥–æ–≤)
    3. –°–æ–∑–¥–∞–µ—Ç/–ø–æ–ª—É—á–∞–µ—Ç —Å–ª–æ–≤–æ –≤ —Ü–µ–ª–µ–≤–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    4. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (keyword, created, removed_count)

    Args:
        profile: –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        category: –¶–µ–ª–µ–≤–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è (ExpenseCategory –∏–ª–∏ IncomeCategory)
        word: –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑–∞
        is_income: True –¥–ª—è –¥–æ—Ö–æ–¥–æ–≤, False –¥–ª—è —Ä–∞—Å—Ö–æ–¥–æ–≤

    Returns:
        (keyword, created, removed_count):
            - keyword: –æ–±—ä–µ–∫—Ç CategoryKeyword –∏–ª–∏ IncomeCategoryKeyword (–∏–ª–∏ None –µ—Å–ª–∏ —Å–ª–æ–≤–æ –∫–æ—Ä–æ—á–µ 3 —Å–∏–º–≤–æ–ª–æ–≤)
            - created: True –µ—Å–ª–∏ —Å–ª–æ–≤–æ —Å–æ–∑–¥–∞–Ω–æ, False –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–ª–æ
            - removed_count: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

    Note:
        –ü–æ–ª–µ 'language' –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è —Ç.–∫. –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.
        CategoryKeyword –∏–º–µ–µ—Ç —ç—Ç–æ –ø–æ–ª–µ –≤ —Å—Ö–µ–º–µ, –Ω–æ –∫–æ–¥ –µ–≥–æ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç.
        IncomeCategoryKeyword –≤–æ–æ–±—â–µ –Ω–µ –∏–º–µ–µ—Ç –ø–æ–ª—è language.
    """
    from expenses.models import CategoryKeyword, IncomeCategoryKeyword

    # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
    KeywordModel = IncomeCategoryKeyword if is_income else CategoryKeyword

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ –æ—á–∏—â–∞–µ–º –æ—Ç stop words
    cleaned_word = prepare_keyword_for_save(word)

    if not cleaned_word or len(cleaned_word) < 3:
        # –°–ª–æ–≤–æ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
        logger.debug(f"Keyword too short after cleaning: '{word}' -> '{cleaned_word}', skipping")
        return None, False, 0

    # –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï max_length=100 (CategoryKeyword.keyword / IncomeCategoryKeyword.keyword)
    # –û–±—Ä–µ–∑–∞–µ–º –ø–æ —Å–ª–æ–≤–∞–º, —á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—Ç—å —Å–ª–æ–≤–∞ –ø–æ—Å–µ—Ä–µ–¥–∏–Ω–µ
    if len(cleaned_word) > 100:
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ 100 —Å–∏–º–≤–æ–ª–æ–≤
        truncated = cleaned_word[:100]
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–æ–±–µ–ª, —á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—Ç—å —Å–ª–æ–≤–æ
        last_space = truncated.rfind(' ')
        if last_space > 0:
            cleaned_word = truncated[:last_space].strip()
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–æ–±–µ–ª–æ–≤ - –æ–±—Ä–µ–∑–∞–µ–º –∂–µ—Å—Ç–∫–æ
            cleaned_word = truncated.strip()

        logger.debug(
            f"Keyword truncated from {len(word)} to {len(cleaned_word)} chars: "
            f"'{cleaned_word}...'"
        )

    # –°–¢–†–û–ì–ê–Ø –£–ù–ò–ö–ê–õ–¨–ù–û–°–¢–¨: —É–¥–∞–ª—è–µ–º —Å–ª–æ–≤–æ –∏–∑ –í–°–ï–• –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    # –ë–ï–ó —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —è–∑—ã–∫—É - —Ç.–∫. –ø–æ–ª–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ production –∫–æ–¥–µ
    deleted = KeywordModel.objects.filter(
        category__profile=profile,
        keyword=cleaned_word
    ).delete()

    removed_count = deleted[0] if deleted else 0

    if removed_count > 0:
        logger.debug(
            f"Removed keyword '{cleaned_word}' from {removed_count} "
            f"{'income' if is_income else 'expense'} categories to maintain uniqueness"
        )

    # –°–æ–∑–¥–∞–µ–º/–ø–æ–ª—É—á–∞–µ–º keyword –≤ —Ü–µ–ª–µ–≤–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    # –ë–ï–ó —É–∫–∞–∑–∞–Ω–∏—è —è–∑—ã–∫–∞ - –ø–æ–ª–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
    keyword, created = KeywordModel.objects.get_or_create(
        category=category,
        keyword=cleaned_word,
        defaults={'usage_count': 0}
    )

    return keyword, created, removed_count
